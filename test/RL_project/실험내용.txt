//2025-10-21
1️⃣ mpcrl/optim/dtr_newton.py
2️⃣ mpcrl/optim/lbfgsb.py
3️⃣ mpcrl/optim/natural_grad.py
위 세 파일을 mpcrl/optim/에 추가함.

q_learning_greenhouse.py에서 
dtr_newton을 이용해 기존 2차 뉴턴형 LSTD Q-learning의 단점(국소 최적해, 불안정한 수렴, Hessian 폭주)을 개선하기 위해
 -> 완화한 “신뢰영역 기반·자연경사형 최적화” 구조를 적용시킴.
 -> 따라서 학습 중 θ 업데이트가 더 안정적이고, 비선형 Q-function에서도 수렴 특성이 부드러워지길 기대.

Damped/Trust-Region Newton (추천 기본값)
초반 30–80 에피소드 구간에서 TD-error 급감, 평가 리워드 빠른 상승
80~120쯤부터 수익률이 둔화 → 200까지 늘려도 증가폭이 작을 때가 많음
목적: “국소 최적해/음수곡률”로 인한 흔들림을 초기에 제어

LBFGS-B
초반 상승은 DTR보다 완만, 대신 100→200에서 꾸준한 추가 개선 기대
메모리·시간 효율이 좋아 장주기 실험에 유리

DiagNaturalGrad
가장 안정적(진동 적음), 느리지만 지속 개선
파라미터 스케일 불일치를 줄여서 100→200 구간에서 이득이 비교적 잘 남

---> 위 과정들 어려울 것 같아서 아래로 변경.
단계	변경 내용	이유	Episode 제안
① Δu 제약 강화	du_lim = 0.1 → 0.07	제어 입력 변화를 제한해 진동 억제	20 ep
	model.py # 수정 (Δu 제약 강화)
	@staticmethod
	def get_du_lim() -> np.ndarray:
    		return 0.07 * Model.get_u_max()  # 한 스텝당 변화폭 30% 감소

	test_80.py # 수정(episode 20)

② R_u 가중치 상승	[10, 1, 1] → [15, 2, 2]	에너지 낭비 억제 + 안정화	20 ep
	test_80.py # 수정
	rl_cost = {"c_u": [15, 2, 2], "c_y": 1.0, "c_dy": 100, "w_y": 1e5*np.ones((1,4))}
	learnable_pars_init["c_u"] = np.array([15, 2, 2])

③ 학습률 감쇠	lr = 0.1 → 0.03, 감쇠 스케줄 적용	과한 파라미터 진동 방지	30 ep
	test_80.py # 수정
	learning_rate = 3e-2  # 0.1 → 0.03
    	optimizer = optim.NetwonMethod(
        	learning_rate=ExponentialScheduler(learning_rate, factor=0.97),  # 점진 감쇠
        	max_percentage_update=0.05,
        	bound_consistency=True,
    	)
	episode = 30

④ 예측지평 확장	H = 24 → 32	더 멀리 내다보는 안정 제어	30 ep
⑤ 총합 파인튜닝	위 세팅 유지, exploration 줄임 (ε ↓)	미세 조정, 최종 수렴	50 ep


FT1~FT4를 각각 독립적으로 학습하고, 각자의 best 파라미터 조합을 식별 → FT5에서 이 네 가지 설정 중 가장 좋은 값들을 조합해서 새 모델을 다시 학습

1번 실험돌린상태 -ing 10/21

2번 실험 돌린상태 -ing 10/27

3번 실험 돌린상태 -ing 10/27
