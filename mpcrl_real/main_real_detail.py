# main_real_detail_clean.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Real-World RL-MPC (CasADi ê¸°ë°˜, ë…¼ë¬¸ ìˆ˜ì‹ (18)~(21))
#   - RL íŒŒë¼ë¯¸í„° ë¡œë“œ & ì£¼ê¸°ì  ì—…ë°ì´íŠ¸
#   - ë³´ìƒ ê³„ì‚°ì€ learning_real_detail ë‚´ë¶€ì—ì„œ ìˆ˜í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import time
import numpy as np
from learning_real_detail import LearningMpcCasADi   # âœ… CasADi MPC
from real_env import RealEnvironment
import sys
import os
import pickle

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§¾ ë¡œê·¸ í´ë” ìë™ ìƒì„± + ë¡œê·¸ íŒŒì¼ ì—´ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("logs", exist_ok=True)
log_path = f"logs/rl_mpc_{time.strftime('%Y%m%d_%H%M%S')}.log"

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, data):
        for f in self.files:
            f.write(data)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

log_file = open(log_path, "w", buffering=1)
sys.stdout = Tee(sys.__stdout__, log_file)
print(f"ğŸ“ Logging to {log_path}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RL-MPC ë©”ì¸ ë£¨í”„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("ğŸš€ Real-world RL-MPC (CasADi) starting...")

    # 1ï¸âƒ£ í™˜ê²½ ì´ˆê¸°í™”
    env = RealEnvironment(sample_time=5.0)
    mpc = LearningMpcCasADi(ts=env.sample_time, N=24)

    # 2ï¸âƒ£ RL í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì ìš© (trained_theta.pkl)
    try:
        mpc.load_theta("trained_theta.pkl")   # âœ… RL í•™ìŠµ ê²°ê³¼ ì ìš©
    except Exception as e:
        print(f"âš ï¸ RL íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    # 3ï¸âƒ£ ì‘ë¬¼ í”„ë¡œí•„ ê¸°ë°˜ ëª©í‘œê°’ ì„¤ì •
    Tmid = sum(env.crop.get("target_temp", [18.0, 22.0])) * 0.5
    Hmid = sum(env.crop.get("target_humidity", [50.0, 70.0])) * 0.5
    mpc.set_reference(Tmid=Tmid, Hmid=Hmid, CO2_ref=420.0, L_ref=300.0)
    print(f"ğŸ¯ Set references â†’ T_ref={mpc.T_ref:.1f}Â°C, H_ref={mpc.H_ref:.1f}%")

    # 4ï¸âƒ£ RL ë²„í¼ ë° ì—…ë°ì´íŠ¸ ì£¼ê¸° ì„¤ì •
    replay_buffer = []
    UPDATE_PERIOD = 3600 * 6   # 6ì‹œê°„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
    last_update = time.time()

    step = 0
    print("âœ… RL-MPC loop running...\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì œì–´ ë£¨í”„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    while True:
        step += 1
        t_now = time.time()

        # (a) ì„¼ì„œ & ì™¸ë€ ì½ê¸°
        x, d = env.read_sensors()
        s = np.concatenate([x, d])

        # (b) MPC ì œì–´ + ë‚´ë¶€ ë³´ìƒ ê³„ì‚°
        u_opt, reward = mpc.policy(s)
        env.send_actuators(u_opt)

        # (c) ë‹¤ìŒ ìƒíƒœ & ê²½í—˜ ì €ì¥
        x_next, d_next = env.read_sensors()
        s_next = np.concatenate([x_next, d_next])
        replay_buffer.append((s, u_opt, reward, s_next))

        # (d) RL íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì£¼ê¸° ì²˜ë¦¬
        if time.time() - last_update > UPDATE_PERIOD:
            print("\nğŸ§  Updating RL parameters (Î¸ = {Q,R,S,Î±}) with "
                  f"{len(replay_buffer)} samples...\n")
            mpc.update_theta(replay_buffer)

            # í•™ìŠµëœ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì €ì¥
            new_theta = {
                "Q": np.diag(mpc.Q).tolist(),
                "R": np.diag(mpc.R).tolist(),
                "S": np.diag(mpc.S).tolist(),
                "alpha_growth": mpc.alpha_growth
            }
            with open("trained_theta.pkl", "wb") as f:
                pickle.dump(new_theta, f)
            print("ğŸ’¾ Updated Î¸ saved to trained_theta.pkl")

            print(f"ğŸ”§ Current Î¸ â†’ Q={np.diag(mpc.Q)}, R={np.diag(mpc.R)}, "
                  f"S={np.diag(mpc.S)}, Î±={mpc.alpha_growth:.3f}\n")

            replay_buffer.clear()
            last_update = time.time()

        # (e) ì œì–´ ì£¼ê¸° ëŒ€ê¸°
        time.sleep(env.sample_time)
