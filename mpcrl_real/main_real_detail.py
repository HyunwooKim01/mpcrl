# main_real_detail.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Real-World MPC (CasADi ê¸°ë°˜, RL ë¶„ë¦¬ ë²„ì „)
#   - ì„¼ì„œ â†’ MPC â†’ ì•¡ì¶”ì—ì´í„° ì œì–´ ë£¨í”„
#   - ë³´ìƒ ë° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ëŠ” ì™¸ë¶€ RL ëª¨ë“ˆ(Q_learning.py)ì—ì„œ ìˆ˜í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import time
import numpy as np
from learning_real_detail import LearningMpcCasADi   # âœ… CasADi MPC
from real_env import RealEnvironment
import sys
import os

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§¾ ë¡œê·¸ í´ë” ìë™ ìƒì„± + íŒŒì¼ ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("logs", exist_ok=True)
log_path = f"logs/mpc_{time.strftime('%Y%m%d_%H%M%S')}.log"

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, data):
        for f in self.files:
            f.write(data)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

log_file = open(log_path, "w", buffering=1)
sys.stdout = Tee(sys.__stdout__, log_file)
print(f"ğŸ“ Logging to {log_path}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Loop
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("ğŸš€ Real-world MPC (CasADi) starting...")

    # 1ï¸âƒ£ í™˜ê²½ ì´ˆê¸°í™”
    env = RealEnvironment(sample_time=5.0)
    mpc = LearningMpcCasADi(ts=env.sample_time, N=24)

    # 2ï¸âƒ£ ì‘ë¬¼ í”„ë¡œí•„ ê¸°ë°˜ ëª©í‘œê°’ ì„¤ì •
    Tmid = sum(env.crop.get("target_temp", [18.0, 22.0])) * 0.5
    Hmid = sum(env.crop.get("target_humidity", [50.0, 70.0])) * 0.5
    mpc.set_reference(Tmid=Tmid, Hmid=Hmid, CO2_ref=420.0, L_ref=300.0)
    print(f"ğŸ¯ Target â†’ T_ref={mpc.T_ref:.1f}Â°C, H_ref={mpc.H_ref:.1f}%")

    # 3ï¸âƒ£ ë£¨í”„ ì„¤ì •
    replay_buffer = []         # Q-learningì—ì„œ ì‚¬ìš© ê°€ëŠ¥
    UPDATE_PERIOD = 3600 * 6   # RL ì—…ë°ì´íŠ¸ ì£¼ê¸°(ì„ íƒ)
    last_update = time.time()
    step = 0

    print("âœ… MPC loop running...\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì œì–´ ë£¨í”„ (RL reward ê³„ì‚° ì œê±°)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    while True:
        step += 1
        t_now = time.time()

        # (a) ì„¼ì„œ & ì™¸ë€ ì½ê¸°
        x, d = env.read_sensors()
        s = np.concatenate([x, d])

        # (b) MPC ì œì–´ ê³„ì‚°
        u_opt = mpc.policy(s)

        # (c) ì•¡ì¶”ì—ì´í„° ëª…ë ¹ ì „ì†¡
        env.send_actuators(u_opt)

        # (d) ìƒíƒœ ë¡œê¹…
        print("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"â± [STEP {step:03d}] t={t_now:.2f} (Î”t={env.sample_time:.1f}s)")
        print(f"ğŸŒ¡ Temp={x[0]:.1f}Â°C  ğŸ’§Hum={x[1]:.1f}%  â˜ï¸COâ‚‚={x[2]:.0f}ppm  ğŸ’¡Light={x[3]:.1f}lx")
        print(f"ğŸŒ Rad={d[0]:.0f}W/mÂ²  ğŸŒ¬OutT={d[2]:.1f}Â°C  ğŸ’§OutH={d[3]:.0f}%")
        print(f"âš™ï¸ u_opt â†’ FAN={u_opt[0]:.2f} | HEATER={u_opt[1]:.2f} | LED={u_opt[2]:.2f}")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

        # (e) ë‹¤ìŒ ìƒíƒœ ì €ì¥ (RL ëª¨ë“ˆìš©)
        x_next, d_next = env.read_sensors()
        s_next = np.concatenate([x_next, d_next])
        replay_buffer.append((s, u_opt, s_next))  # reward ì œì™¸

        # (f) RL ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±° (ì„ íƒ)
        if time.time() - last_update > UPDATE_PERIOD:
            # ì™¸ë¶€ Q_learning.pyê°€ Î¸ ì—…ë°ì´íŠ¸ ë‹´ë‹¹
            replay_buffer.clear()
            last_update = time.time()

        # (g) ì œì–´ ì£¼ê¸° ëŒ€ê¸°
        time.sleep(env.sample_time)
